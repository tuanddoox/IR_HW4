{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from ltr.utils import seed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Counterfactual LTR (130 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ltr.dataset import load_data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that there is a logging policy that shows the results for each query to the users and logs the user clicks.\n",
    "For that, we provide a logging policy simulator `LoggingPolicy`.\n",
    "Our logging policy only shows top 20 documents to the users.\n",
    "You can use this simulator to:\n",
    "- Get the position of the documents for a query in the SERP: `query_positions`.\n",
    "- Gather the (simulated) clicks of users for a query: `gather_clicks`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99]\n",
      "[17 29 25 94 99 21 22 68  2 86 31 44 47 88 55 76 97 45 81 35 58 74 72 61\n",
      "  5 83 96 64 49 27 28 75  3  1 51 36 18 26  8 79 33 71 69 56 20 59  9 98\n",
      " 63 92 73 62 50 70 24 13 46 65 42 54 12 52 37 87 90 43 95 84 10  7 34 57\n",
      " 77 19 82 67 78 39 32 60 30 14 89 11 53  6 41 16 38 15 93 91 40 85 48 66\n",
      " 80 23  4  0]\n",
      "clicks for session 1 on documents [99] on positions [0]\n"
     ]
    }
   ],
   "source": [
    "from ltr.logging_policy import LoggingPolicy\n",
    "\n",
    "logging_policy = LoggingPolicy()\n",
    "\n",
    "# Gather the clicks on the SERP for query 20\n",
    "for i in range(10):\n",
    "    clicked_docs = np.where(logging_policy.gather_clicks(20))[0] \n",
    "    clicked_positions = logging_policy.query_positions(20)[clicked_docs] \n",
    "    print(f'clicks for session {i+1} on documents', clicked_docs, 'on positions', clicked_positions)\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Utils (10 points)\n",
    "\n",
    "### Click data loader (10 points)\n",
    "First, we need to have a data loader that feeds the model with features and click data.\n",
    "In this data loader, you have to select `topk=20` items for each query, and return three tensors:\n",
    "- Feature vectors of the selected documents,\n",
    "- One instance of the clicks over the selected documents, using the `gather_clicks(qid)` function, and\n",
    "- The positions of the selected documents in the SERP.\n",
    "\n",
    "**IMPORTANT** Here you *should not* use the `labels` for training. It is assumed that we cannot observe the real labels and want to use the `clicks` to train our LTR model instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 15]) torch.Size([1, 20]) torch.Size([1, 20])\n",
      "clicks: tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "positions: tensor([[15,  3, 18,  0, 11,  7, 17,  9,  4, 10, 19,  2, 12, 13,  8, 16,  5, 14,\n",
      "          6,  1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuanddoox/Documents/UvA/study/IR1_23/Assignment/assignment2-part2-team-54/ltr/dataset.py:360: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1675740396714/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  tensor_features = torch.Tensor(features_topk)\n"
     ]
    }
   ],
   "source": [
    "from ltr.dataset import ClickLTRData\n",
    "\n",
    "train_dl = DataLoader(ClickLTRData(data, logging_policy), batch_size=1, shuffle=True)\n",
    "\n",
    "for features, clicks, positions in train_dl:\n",
    "    assert positions.dtype == torch.long\n",
    "    print(features.shape, clicks.shape, positions.shape)\n",
    "    print('clicks:', clicks)\n",
    "    print('positions:', positions)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DELETE \n",
    "\n",
    "import pytest\n",
    "from ltr.dataset import DataFoldSplit\n",
    "\n",
    "def get_mock_data():\n",
    "    class MockData():\n",
    "        def __init__(self) -> None:\n",
    "            np.random.seed(110)\n",
    "            self.train = DataFoldSplit(\n",
    "                self,\n",
    "                \"train\",\n",
    "                np.array([  0, 100, 193, 278]),\n",
    "                np.random.rand(278,15),\n",
    "                np.array(\n",
    "                    [4, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
    "                    0, 0, 0, 0, 0, 3, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4,\n",
    "                    0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3,\n",
    "                    0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0,\n",
    "                    0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 1, 1, 4, 0, 1, 4, 0, 0, 0, 0, 0, 0,\n",
    "                    0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0,\n",
    "                    0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0,\n",
    "                    0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                    0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                    0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
    "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "            )\n",
    "            self.validation = self.train\n",
    "            self.test = self.train\n",
    "\n",
    "    return MockData()\n",
    "\n",
    "def test_ClickLTRData(get_mock_data):\n",
    "    from ltr.logging_policy import LoggingPolicy\n",
    "    from ltr.dataset import ClickLTRData\n",
    "\n",
    "    data = get_mock_data()\n",
    "    logging_policy = LoggingPolicy()\n",
    "\n",
    "    click_data = ClickLTRData(data, logging_policy)\n",
    "\n",
    "    assert isinstance(click_data[0][0], torch.FloatTensor)\n",
    "    assert click_data[0][0].shape == torch.Size((20,15))\n",
    "\n",
    "    ctr = 0\n",
    "    for i in range(1000):\n",
    "        ctr += click_data[1][1].sum()\n",
    "    assert (ctr > 1800) & (ctr < 2300)\n",
    "\n",
    "    assert torch.allclose(click_data[2][2], torch.LongTensor([18, 10,  9,  2,  6,  0,  4,  7, 11,  1,  5,  8, 12, 13, 14,  3, 15, 19,\n",
    "        16, 17]))\n",
    "\n",
    "test_ClickLTRData(get_mock_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTR model (0 points!)\n",
    "Further, let's modify the `LTRModel` from previous chapter and take the width of the middle layer as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTRModel(\n",
      "  (layers): Sequential(\n",
      "    (layer1): Linear(in_features=15, out_features=20, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (out): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from ltr.model import LTRModel\n",
    "\n",
    "net = LTRModel(data.num_features, width=20)\n",
    "print(net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ListNet (40 points)\n",
    "\n",
    "In the previous chapter, you have implemented different loss functions for LTR.\n",
    "Here we use another well known listwise loss funtion, called `ListNet`, and will use it for our unbiased LTR model.\n",
    "The idea behind ListNet is very simple:\n",
    "To solve the discontinuity issue of NDCG, in **ListNet**, the loss function is based on probability distribution on permutations.\n",
    "\n",
    "Define a family of distributions on permutation of scores $z$, $P_z(\\pi)$, s.t. $\\sum_{\\pi\\in\\Omega} P_z(\\pi)=1$, where $\\Omega$ is the set of all $n!$ permutations.\n",
    "Ideally, we want the scores of our LTR model lead to the same permutation distribution as the labels $y$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min KL(P_y,P_z)=-\\sum_{\\pi\\in\\Omega} P_y(\\pi) \\log P_z(\\pi)\n",
    "$$\n",
    "\n",
    "Plackett-Luce distribution gives a general formula for calculating the permutation distribution:\n",
    "\n",
    "$$\n",
    "P_z(\\pi) = \\prod_{j=1}^{n} \\frac{\\exp(z_{\\pi(j)})}{\\sum_{k=j}^{n} \\exp(z_{\\pi(k)})}\n",
    "$$\n",
    "In ListNet, instead of calculating $n!$ permutation probabilities, the top one probability of each document is calculated:\n",
    "\n",
    "$$\n",
    "P_z(j) = \\sum_{\\pi(1)=j, \\pi\\in\\Omega} P_z(\\pi) = \\frac{\\exp(z_{j})}{\\sum_{k=1}^{n} \\exp(z_{k})},\n",
    "$$\n",
    "which is the softmax function.\n",
    "\n",
    "Then, the loss is defined as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{ListNet}}=-\\sum_{j=1}^{n} P_y(j) \\log P_z(j),\n",
    "$$\n",
    "where the softmax function is used to calculate $P_y(j)$ and $P_z(j)$ from the labels and predictions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tensor([[[-0.5],[-.4],[-.5],[ .002],[ 2.2],[ 7],[ 1.8],[ 6.5],\n",
    "                            [ 6.8],[ 2.3],[-.3],[ .8],[-.3],[-.6],[ .005],[-.5],\n",
    "                            [ .002],[ .2],[-.6],[-.07]]])\n",
    "# output = torch.tensor([[[0.8176176],  [0.08738231], [0.09500005]]])\n",
    "# clicks1 = torch.tensor([[0.8437947,  0.11419519, 0.04201007]])\n",
    "clicks1 = torch.tensor([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.]])\n",
    "clicks2 = torch.tensor([[0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ListNet loss function (20 points)\n",
    "Implement the ListNet loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 15]) torch.Size([1, 20]) torch.Size([1, 20])\n",
      "torch.Size([1, 20, 1]) torch.Size([1, 20])\n",
      "tensor(8.3041, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from ltr.loss import listNet_loss\n",
    " \n",
    "biased_net = LTRModel(data.num_features, width=20)\n",
    "\n",
    "for features, clicks, positions in train_dl:\n",
    "    print(features.shape, clicks.shape, positions.shape)\n",
    "    output = biased_net(features)\n",
    "    print(output.shape, clicks.shape)\n",
    "    loss = listNet_loss(output, clicks)\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biased ListNet training (10 points)\n",
    "Now use `listNet_loss` to train an LTR model. Since we use `clicks` instead of `relevance`, and do not correct for the bias, this would be a biased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import train_biased_listNet\n",
    "\n",
    "params = Namespace(epochs=1, \n",
    "                    lr=1e-4,\n",
    "                    batch_size=1,\n",
    "                    metrics={\"ndcg@10\", \"precision@10\", \"recall@10\"})\n",
    "\n",
    "biased_net = LTRModel(15, width=20)\n",
    "train_biased_listNet(biased_net, params, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results (10 points - no implementation!)\n",
    "Since we randomly simulate clicks and use them to train our model, for the evaluation we train and save 10 different models and inspect the average and std over them.\n",
    "\n",
    "**IMPORTANT** Run the following cell to store your models and results. After it finishes, make sure to push the results to the git repo.\n",
    "\n",
    "_Estimated time on Codespaces_: 5m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.utils import create_results\n",
    "from ltr.train import train_biased_listNet\n",
    "\n",
    "seed(42)\n",
    "params = Namespace(epochs=20, \n",
    "                    lr=1e-4,\n",
    "                    batch_size=1,\n",
    "                    metrics={\"ndcg@10\", \"precision@10\", \"recall@10\"})\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Training Model', i)\n",
    "    biased_net = LTRModel(15, width=20)\n",
    "    create_results(data, biased_net, \n",
    "                train_biased_listNet, \n",
    "                biased_net,\n",
    "                f\"./outputs/biased_listNet_{i}.json\",\n",
    "                params)\n",
    "\n",
    "    torch.save(biased_net.state_dict(), f\"./outputs/biased_listNet_{i}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Unbiased ListNet (30 points)\n",
    "\n",
    "### Unbiased ListNet loss function (10 points)\n",
    "\n",
    "Now, we use IPS to have an unbiased ListNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 15]) torch.Size([1, 20]) torch.Size([1, 20])\n",
      "torch.Size([1, 20, 1]) torch.Size([1, 20])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 31 is out of bounds for dimension 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m output \u001b[39m=\u001b[39m biased_net(features)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape, clicks\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 12\u001b[0m \u001b[39mprint\u001b[39m(propensity[positions\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mnumpy()])\n\u001b[1;32m     13\u001b[0m loss \u001b[39m=\u001b[39m unbiased_listNet_loss(output, clicks, propensity[positions\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy()])\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(loss)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 31 is out of bounds for dimension 0 with size 20"
     ]
    }
   ],
   "source": [
    "from ltr.loss import unbiased_listNet_loss\n",
    "\n",
    "unbiased_net = LTRModel(data.num_features, width=20)\n",
    "propensity = logging_policy.propensity\n",
    "\n",
    "\n",
    "\n",
    "for features, clicks, positions in train_dl:\n",
    "    print(features.shape, clicks.shape, positions.shape)\n",
    "    output = biased_net(features)\n",
    "    print(output.shape, clicks.shape)\n",
    "    print(propensity[positions.data.numpy()])\n",
    "    loss = unbiased_listNet_loss(output, clicks, propensity[positions.data.numpy()])\n",
    "    print(loss)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbiased ListNet training (10 points)\n",
    "Now use `unbiased_listNet_loss` to train an LTR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import train_unbiased_listNet\n",
    "\n",
    "params = Namespace(epochs=1, \n",
    "                    lr=1e-4,\n",
    "                    batch_size=1,\n",
    "                    propensity=logging_policy.propensity,\n",
    "                    metrics={\"ndcg@10\", \"precision@10\", \"recall@10\"})\n",
    "\n",
    "biased_net = LTRModel(15, width=20)\n",
    "train_unbiased_listNet(biased_net, params, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results (10 points - no implementation!)\n",
    "Similar to the biased model, here we train 10 different unbiased models and save them to inspect the average and std over them.\n",
    "\n",
    "**IMPORTANT** Run the following cell to store your models and results. After it finishes, make sure to push the results to the git repo.\n",
    "\n",
    "_Estimated time on Codespaces_: 5m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.utils import create_results\n",
    "from ltr.train import train_unbiased_listNet\n",
    "\n",
    "seed(42)\n",
    "params = Namespace(epochs=20, \n",
    "                    lr=1e-4,\n",
    "                    batch_size=1,\n",
    "                    propensity=logging_policy.propensity,\n",
    "                    metrics={\"ndcg@10\", \"precision@10\", \"recall@10\"})\n",
    "\n",
    "for i in range(10):\n",
    "    print('Training Model', i)\n",
    "    unbiased_net = LTRModel(15, width=20)\n",
    "    create_results(data, unbiased_net, \n",
    "                train_unbiased_listNet, \n",
    "                unbiased_net,\n",
    "                f\"./outputs/unbiased_listNet_{i}.json\",\n",
    "                params)\n",
    "\n",
    "    torch.save(unbiased_net.state_dict(), f\"./outputs/unbiased_listNet_{i}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Propensity estimation (35 points)\n",
    "\n",
    "In training our unbiased ListNet model, we assumed that we know propensity values.\n",
    "In practice, however, the propensity values have to be estimated from the clicks.\n",
    "There are several methods for estimating the propensities, such as dual learning algorithm (DLA) and regression-based EM.\n",
    "Here, we focus on DLA.\n",
    "\n",
    "### DLA\n",
    "\n",
    "IPS is based on the examination hypothesis that says $P(c=1)=P(r=1)\\times P(e=1)$, where $c$, $r$ and $e$ are click, relevance and examination signals, respectively.\n",
    "Initially, we are interested in $P(r=1)$, so in IPS we substitute $c$ with $\\hat{r}=\\frac{c}{P(e=1)}$.\n",
    "In practice, $P(e=1)$ is not given and should be estimated.\n",
    "DLA solves this by noticing that $\\hat(e)=\\frac{r}{P(r=1)}$ is also an unbiased estimation for the examination probability.\n",
    "This means that in DLA (as the name suggests), two models are trained at the same time:\n",
    "- Relevance prediction: A function $f$, modeled by `LTRModel` here, that estimates the relevance from the feature vectors.\n",
    "- Propensity prediction: A function $g$, modeled by `PropLTRModel` here, that estimates the propensity from the positions.\n",
    "\n",
    "Using the `unbiased_listNet_loss` loss function with the following signature:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{unbiased}}\\big(\\text{predictions}, \\text{clicks}, \\text{propensities}\\big),\n",
    "$$\n",
    "\n",
    "the overall loss function is as follows:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DLA}} = \\underbrace{\\mathcal{L}_{\\text{unbiased}}\\bigg(f(x), c, \\sigma\\big(g(p)\\big)\\bigg)}_{\\text{relevance estimation}} + \\underbrace{\\mathcal{L}_{\\text{unbiased}} \\bigg(g(p), c, \\sigma\\big(f(x)\\big)\\bigg)}_{\\text{propensity estimation}},\n",
    "$$\n",
    "which means that the predictions of $g$ are used as the propensities for optimizing $f$, and the predictions of $f$ are used as the propensities for optimizing $g$.\n",
    "The $\\sigma()$ function is used to transform the logits to valid probability valules, as the propensities should be between 0 and 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits to prob (2 points)\n",
    "First, we need a function to transform the logits to valid probability values (between 0 and 1).\n",
    "Use the sigmoid function for this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import logit_to_prob\n",
    "\n",
    "logits = 10 * torch.rand(10)\n",
    "probs = logit_to_prob(logits)\n",
    "\n",
    "# Print the propensities\n",
    "print('probabilities:', probs.squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity estimation LTR model (3 points)\n",
    "Then, we need a wrapper around the `LTRModel` that takes as input the positions (Long tensor) and outputs the logits for propensities.\n",
    "This new model uses one hot embedding as the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.model import PropLTRModel\n",
    "\n",
    "prop_net = PropLTRModel(logging_policy.topk, width=200)\n",
    "\n",
    "logits = prop_net(torch.arange(17))\n",
    "probs = logit_to_prob(logits)\n",
    "\n",
    "# Print the propensities\n",
    "print('probabilities:', probs.T)\n",
    "\n",
    "# Print the normalized propensities\n",
    "print('normalized with the first position:', probs.T/probs.squeeze()[0])        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLA training (20 points)\n",
    "Now we have all we need for the DLA implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import train_DLA_listNet\n",
    "\n",
    "params = Namespace(epochs=5, \n",
    "                    lr=1e-4,\n",
    "                    batch_size=1,\n",
    "                    prop_lr=1e-3,\n",
    "                    prop_net=PropLTRModel(logging_policy.topk, width=256),\n",
    "                    metrics={\"ndcg@10\", \"precision@10\", \"recall@10\"})\n",
    "\n",
    "biased_net = LTRModel(15, width=256)\n",
    "print('True (unknown to the model) propensities:', logging_policy.propensity.data.numpy())\n",
    "train_DLA_listNet(biased_net, params, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results (10 points - no implementation!)\n",
    "Similar to the biased model, here we train 10 different unbiased models and save them to inspect the average and std over them.\n",
    "\n",
    "**IMPORTANT** Run the following cell to store your models and results. After it finishes, make sure to push the results to the git repo.\n",
    "\n",
    "_Estimated time on Codespaces_: < 10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.utils import create_results\n",
    "from ltr.train import train_DLA_listNet\n",
    "\n",
    "seed(42)\n",
    "params = Namespace(epochs=20, \n",
    "                    lr=1e-4,\n",
    "                    batch_size=1,\n",
    "                    prop_lr=1e-3,\n",
    "                    prop_net=None,\n",
    "                    metrics={\"ndcg@10\", \"precision@10\", \"recall@10\"})\n",
    "\n",
    "for i in range(10):\n",
    "    print('Training Model', i)\n",
    "    dla_net = LTRModel(15, width=256)\n",
    "    params.prop_net = PropLTRModel(logging_policy.topk, width=256)\n",
    "    create_results(data, dla_net, \n",
    "                train_DLA_listNet, \n",
    "                dla_net,\n",
    "                f\"./outputs/DLA_listNet_{i}.json\",\n",
    "                params)\n",
    "\n",
    "    torch.save(dla_net.state_dict(), f\"./outputs/DLA_listNet_{i}\")\n",
    "    torch.save(params.prop_net.state_dict(), f\"./outputs/DLA_listNet_prop_{i}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing the models (15 points)\n",
    "\n",
    "You have implemented three models: biased, unbiased with oracle propensity values, and unbiased with DLA-estimated propensity values.\n",
    "Given the training results and evaluation results, please elaborate on the ranking performance of these three models in `analysis.md`. See that file for further details.\n",
    "\n",
    "Note that you need to submit the result files created in `outputs/` for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def aggregate_results(model_name):\n",
    "    aggregated_metrics = {}\n",
    "    for i in range(10):\n",
    "        with open(f\"./outputs/{model_name}_{i}.json\", \"r\") as reader:\n",
    "            result = json.load(reader)\n",
    "            for metric, (v, std) in result['test_metrics'].items():\n",
    "                aggregated_metrics.setdefault(metric, []).append(v)\n",
    "    return {metric: np.mean(vals) for metric, vals in aggregated_metrics.items()}\n",
    "\n",
    "biased = aggregate_results('biased_listNet')\n",
    "unbiased = aggregate_results('unbiased_listNet')\n",
    "DLA = aggregate_results('DLA_listNet')\n",
    "\n",
    "# save the aggregated output files\n",
    "for model_avg_results, model_name in zip([biased, unbiased, DLA], [\"biased_listNet\", \"unbiased_listNet\", \"DLA_listNet\"]):\n",
    "    json.dump(model_avg_results, open(f\"outputs/{model_name}_avg.json\", \"wt\"))\n",
    "\n",
    "# display a handful of metrics\n",
    "print_metrics = [\"ndcg\", \"ndcg@20\", \"precision@05\", \"recall@20\"]\n",
    "print_biased = {metric: v for metric, v in biased.items() if metric in print_metrics}\n",
    "print_unbiased = {metric: v for metric, v in unbiased.items() if metric in print_metrics}\n",
    "print_DLA = {metric: v for metric, v in DLA.items() if metric in print_metrics}\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "df = pd.DataFrame([print_biased, print_unbiased, print_DLA], index=[\"biased\", \"unbiased\", \"DLA\"])\n",
    "print(df)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to submit your outputs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR1_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2c5ccf7e153c3826497608a13106df1b3c5c34cecf259281315c8b06c776444"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
